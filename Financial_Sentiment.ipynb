{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033b6ac8-1e1b-481a-af32-b1d37a5a677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from string import punctuation\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12889bcf-0fae-495f-a2aa-7e0e38e6f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('finance_dataset/positive_sentiment.csv')\n",
    "df2 = pd.read_csv('finance_dataset/negative_sentiment.csv')\n",
    "\n",
    "df2.label = 0\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e527316-8252-4396-a96a-a875555c39de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5989 entries, 0 to 5988\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5989 non-null   object\n",
      " 1   label   5989 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 93.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc50d7f4-24bc-44c6-a83c-3c216567b350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    4526\n",
       "0    1463\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1a492c-ec0f-4881-a0a5-fe3fe70e806d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>britam pretax profit dips 80 due to 2016 reval...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nys to provide sh50 psv services in nairobi ro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenya welcomes sh101b renewable energy facilit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kcb to fly two lucky customers to watch fifa w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41 of kenyans depend on raw water sources shoa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  britam pretax profit dips 80 due to 2016 reval...      1\n",
       "1  nys to provide sh50 psv services in nairobi ro...      1\n",
       "2  kenya welcomes sh101b renewable energy facilit...      1\n",
       "3  kcb to fly two lucky customers to watch fifa w...      1\n",
       "4  41 of kenyans depend on raw water sources shoa...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a160a0-be4e-4018-943b-de6d37f8a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which performs tokenization, lemmatization and removes stop words and punctuations\n",
    "# This step is important as it removes any unnecessary data\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word not in punctuation]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb250c5-a64c-487b-8847-efc0d8807386",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AUDRIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AUDRIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AUDRIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')  \n",
    "nltk.download('wordnet')     \n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947f3e00-3448-4990-9813-2c4c1015f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f26ed1-c366-430e-8f02-734386f80498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training, validation and testing data\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "smote = SMOTE()\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(df.processed_text, df.label, test_size = 0.35, stratify=df.label, random_state = 123)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size = 0.35, stratify=y_temp, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb316974-9c80-4967-9bcd-321919ca53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization of the data and padding to ensure that the length of the arrays is identical\n",
    "\n",
    "max_features = 1000\n",
    "tokenizer = Tokenizer(num_words = max_features, split = ' ')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(training_sequences, maxlen = 500, padding = 'post')\n",
    "\n",
    "validation_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "X_val_padded = pad_sequences(validation_sequences, maxlen = 500, padding = 'post')\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(testing_sequences, maxlen = 500, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4b220db-448b-4236-9c8a-321d1811604b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1957    oil discovered kenya recoverable deposit surpa...\n",
      "3923                                               wti 51\n",
      "5031     nigeria loses 100 billion revenue attack cut oil\n",
      "2064    east africa aim regional stock exchange within...\n",
      "303           rising need conveniencefocused business via\n",
      "                              ...                        \n",
      "2982    ivory coast raise 750m 10year bond sale deal m...\n",
      "2477    ghana cut key interest rate halfpoint inflatio...\n",
      "3058    fund available spending public service grow av...\n",
      "1488    uganda cut benchmark lending rate 90 country 1...\n",
      "5783    junction mall statement regarding decision eje...\n",
      "Name: processed_text, Length: 3892, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "261caadb-da84-4f24-be75-c9690e0e7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SMOTE to handle imbalance in the data by synthetically creating values for the minority class\n",
    "\n",
    "X_train_sm, y_train_sm = smote.fit_resample(X_train_padded, y_train)\n",
    "X_val_sm, y_val_sm = smote.fit_resample(X_val_padded, y_val)\n",
    "X_test_sm, y_test_sm = smote.fit_resample(X_test_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aaca636-f5a4-4c94-a279-a76897a52b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    2941\n",
      "0    2941\n",
      "Name: count, dtype: int64 label\n",
      "1    555\n",
      "0    555\n",
      "Name: count, dtype: int64 label\n",
      "1    1030\n",
      "0    1030\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train_sm.value_counts(), y_val_sm.value_counts(), y_test_sm.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3aa34b6-d7d2-4a8a-bd0a-bfa8141efeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 35)           256200    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 500, 128)         51200     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 500, 128)         98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 517,513\n",
      "Trainable params: 517,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Creating the model and adding the necessary layers\n",
    "\n",
    "embedding_dim = 35\n",
    "max_len = 500\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(total_words, embedding_dim, input_length=max_len),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True, recurrent_activation='sigmoid')), \n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True, recurrent_activation='sigmoid')),  \n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64, recurrent_activation='sigmoid')),\n",
    "    keras.layers.Dense(64, activation='sigmoid'),\n",
    "    keras.layers.Dense(64, activation='sigmoid'),\n",
    "    keras.layers.Dense(1, activation='sigmoid') \n",
    "])\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c13eef83-36c3-4e65-a712-ef6b68d9a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "184/184 [==============================] - 346s 2s/step - loss: 0.6890 - accuracy: 0.5350 - val_loss: 0.6447 - val_accuracy: 0.6577\n",
      "Epoch 2/25\n",
      "184/184 [==============================] - 308s 2s/step - loss: 0.5867 - accuracy: 0.7052 - val_loss: 0.6078 - val_accuracy: 0.7009\n",
      "Epoch 3/25\n",
      "184/184 [==============================] - 381s 2s/step - loss: 0.5180 - accuracy: 0.7577 - val_loss: 0.5613 - val_accuracy: 0.7261\n",
      "Epoch 4/25\n",
      "184/184 [==============================] - 370s 2s/step - loss: 0.4610 - accuracy: 0.8041 - val_loss: 0.5666 - val_accuracy: 0.7270\n",
      "Epoch 5/25\n",
      "184/184 [==============================] - 365s 2s/step - loss: 0.4192 - accuracy: 0.8307 - val_loss: 0.5862 - val_accuracy: 0.7324\n",
      "Epoch 6/25\n",
      "184/184 [==============================] - 401s 2s/step - loss: 0.3905 - accuracy: 0.8512 - val_loss: 0.6363 - val_accuracy: 0.7234\n",
      "Epoch 7/25\n",
      "184/184 [==============================] - 400s 2s/step - loss: 0.3600 - accuracy: 0.8667 - val_loss: 0.6388 - val_accuracy: 0.7144\n",
      "Epoch 8/25\n",
      "184/184 [==============================] - 408s 2s/step - loss: 0.3441 - accuracy: 0.8762 - val_loss: 0.6507 - val_accuracy: 0.7207\n",
      "Epoch 9/25\n",
      "184/184 [==============================] - 407s 2s/step - loss: 0.3311 - accuracy: 0.8841 - val_loss: 0.7166 - val_accuracy: 0.7153\n",
      "Epoch 10/25\n",
      "184/184 [==============================] - 424s 2s/step - loss: 0.3250 - accuracy: 0.8861 - val_loss: 0.6787 - val_accuracy: 0.7225\n",
      "Epoch 11/25\n",
      "184/184 [==============================] - 604s 3s/step - loss: 0.3095 - accuracy: 0.8922 - val_loss: 0.6937 - val_accuracy: 0.7306\n",
      "Epoch 12/25\n",
      "184/184 [==============================] - 515s 3s/step - loss: 0.2797 - accuracy: 0.9019 - val_loss: 0.7193 - val_accuracy: 0.7234\n",
      "Epoch 13/25\n",
      "184/184 [==============================] - 321s 2s/step - loss: 0.2753 - accuracy: 0.9012 - val_loss: 0.7388 - val_accuracy: 0.7279\n",
      "Epoch 14/25\n",
      "184/184 [==============================] - 300s 2s/step - loss: 0.2623 - accuracy: 0.9053 - val_loss: 0.7625 - val_accuracy: 0.7234\n",
      "Epoch 15/25\n",
      "184/184 [==============================] - 303s 2s/step - loss: 0.2386 - accuracy: 0.9121 - val_loss: 0.7861 - val_accuracy: 0.7189\n",
      "Epoch 16/25\n",
      "184/184 [==============================] - 290s 2s/step - loss: 0.2093 - accuracy: 0.9283 - val_loss: 0.7916 - val_accuracy: 0.7117\n",
      "Epoch 17/25\n",
      "184/184 [==============================] - 281s 2s/step - loss: 0.1895 - accuracy: 0.9398 - val_loss: 0.8618 - val_accuracy: 0.7180\n",
      "Epoch 18/25\n",
      "184/184 [==============================] - 281s 2s/step - loss: 0.1882 - accuracy: 0.9396 - val_loss: 0.8326 - val_accuracy: 0.7072\n",
      "Epoch 19/25\n",
      "184/184 [==============================] - 263s 1s/step - loss: 0.1796 - accuracy: 0.9441 - val_loss: 0.8809 - val_accuracy: 0.7090\n",
      "Epoch 20/25\n",
      "184/184 [==============================] - 292s 2s/step - loss: 0.1628 - accuracy: 0.9551 - val_loss: 0.8887 - val_accuracy: 0.7135\n",
      "Epoch 21/25\n",
      "184/184 [==============================] - 279s 2s/step - loss: 0.1460 - accuracy: 0.9600 - val_loss: 0.9407 - val_accuracy: 0.7144\n",
      "Epoch 22/25\n",
      "184/184 [==============================] - 292s 2s/step - loss: 0.1473 - accuracy: 0.9599 - val_loss: 0.9005 - val_accuracy: 0.7270\n",
      "Epoch 23/25\n",
      "184/184 [==============================] - 288s 2s/step - loss: 0.1441 - accuracy: 0.9606 - val_loss: 0.9303 - val_accuracy: 0.7180\n",
      "Epoch 24/25\n",
      "184/184 [==============================] - 277s 2s/step - loss: 0.1338 - accuracy: 0.9646 - val_loss: 0.9359 - val_accuracy: 0.7153\n",
      "Epoch 25/25\n",
      "184/184 [==============================] - 276s 2s/step - loss: 0.1250 - accuracy: 0.9674 - val_loss: 0.9817 - val_accuracy: 0.7171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ef9931d0a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_sm, y_train_sm, validation_data = (X_val_sm, y_val_sm), epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5bee893-515f-40d1-8cda-1e61a85765be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 [==============================] - 18s 425ms/step - loss: 1.0123 - accuracy: 0.7190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0123087167739868, 0.7190021872520447]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_padded, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df3c4c9-7097-4c93-ab0e-e2305b9f81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function which will be used to predict the sentiment for the input text\n",
    "\n",
    "def predict(text, tokenizer, model):\n",
    "    # Preprocess the text\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word not in punctuation]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    processed_text = \" \".join(lemmatized_tokens)\n",
    "    \n",
    "    # Convert the processed text into a sequence of integers using the trained tokenizer\n",
    "    sequences = tokenizer.texts_to_sequences([processed_text])  # Note the brackets\n",
    "    predict_padded = pad_sequences(sequences, maxlen=500, padding='post')\n",
    "\n",
    "    # Model prediction\n",
    "    predicted_sentiment = model.predict(predict_padded)\n",
    "    \n",
    "    # Convert probability to binary class (0 or 1)\n",
    "    predicted_class = int(np.round(predicted_sentiment[0][0]))  # Round to get either 0 or 1\n",
    "    \n",
    "    if (predicted_class == 0):\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33b3dcfd-3c50-4e1a-aa99-8c58a16ff86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Negative'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The outlook is not looking great\"\n",
    "\n",
    "predict(text, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d41a4db-ec25-4513-ba31-9b28ff756515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 75ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"The value of the stocks are going up and people are investing\",  tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8c80d19-0ca7-4985-b2b4-ec4f6b436be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists(\"finance_dataset\"):\n",
    "    os.makedirs(\"finance_dataset\")\n",
    "\n",
    "model_versions = [i for i in os.listdir(\"finance_dataset\") if i.isdigit()]\n",
    "\n",
    "model_version = max([int(i) for i in model_versions] + [0]) + 1\n",
    "\n",
    "model.save(f\"finance_dataset/model{model_version}.h5\")\n",
    "\n",
    "# model.save('model.h5')\n",
    "\n",
    "save_path = 'finance_dataset'\n",
    "tokenizer_path = os.path.join(save_path, 'tokenizer.json')\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(tokenizer_path, 'w') as f:\n",
    "    f.write(tokenizer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d214e-6d31-47cc-b299-1dc00c8ed656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
